{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXYHCvuqRAeP",
        "outputId": "c0bdd1b2-a6c1-493d-e725-f5298eb35a79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [0.9999999999999992]\n",
            "Intercept: 15.000000000000009\n"
          ]
        }
      ],
      "source": [
        "# Example: Linear Regression with Spark MLlib\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName('MLlib Example').getOrCreate()\n",
        "\n",
        "# Load sample data\n",
        "data = [(1, 5.0, 20.0), (2, 10.0, 25.0), (3, 15.0, 30.0), (4, 20.0, 35.0)]\n",
        "columns = ['ID', 'Feature', 'Target']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Prepare data for modeling\n",
        "assembler = VectorAssembler(inputCols=['Feature'], outputCol='Features')\n",
        "df_transformed = assembler.transform(df)\n",
        "\n",
        "# Train a linear regression model\n",
        "lr = LinearRegression(featuresCol='Features', labelCol='Target')\n",
        "model = lr.fit(df_transformed)\n",
        "\n",
        "# Print model coefficients\n",
        "print(f'Coefficients: {model.coefficients}')\n",
        "print(f'Intercept: {model.intercept}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"LogisticRegressionExample\").getOrCreate()\n",
        "\n",
        "# Example dataset: Convert `Features` to Vectors\n",
        "data = [\n",
        "    (1, Vectors.dense([2.0, 3.0]), 0),\n",
        "    (2, Vectors.dense([1.0, 5.0]), 1),\n",
        "    (3, Vectors.dense([2.5, 4.5]), 1),\n",
        "    (4, Vectors.dense([3.0, 6.0]), 0),\n",
        "]\n",
        "columns = ['ID', 'Features', 'Label']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# No need for VectorAssembler since Features are already in Vector format\n",
        "# Train logistic regression model\n",
        "lr = LogisticRegression(featuresCol='Features', labelCol='Label')\n",
        "model = lr.fit(df)\n",
        "\n",
        "# Display coefficients and intercept\n",
        "print(f'Coefficients: {model.coefficients}')\n",
        "print(f'Intercept: {model.intercept}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41GGC-gQRBsG",
        "outputId": "ba30f486-8b7e-4ff4-864b-0085442582bd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [-12.262057929180484,4.087352266486688]\n",
            "Intercept: 11.56891272665312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"KMeansExample\").getOrCreate()\n",
        "\n",
        "# Example dataset: Convert `Features` to Vectors\n",
        "data = [\n",
        "    (1, Vectors.dense([1.0, 1.0])),\n",
        "    (2, Vectors.dense([5.0, 5.0])),\n",
        "    (3, Vectors.dense([10.0, 10.0])),\n",
        "    (4, Vectors.dense([15.0, 15.0])),\n",
        "]\n",
        "columns = ['ID', 'Features']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Train KMeans clustering model\n",
        "kmeans = KMeans(featuresCol='Features', k=2)\n",
        "model = kmeans.fit(df)\n",
        "\n",
        "# Show cluster centers\n",
        "centers = model.clusterCenters()\n",
        "print(f'Cluster Centers: {centers}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L805IYVSShQb",
        "outputId": "ba2d8a60-e89b-4f73-ba11-ddf7cf045174"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster Centers: [array([12.5, 12.5]), array([3., 3.])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "\n",
        "# Membuat sesi Spark\n",
        "spark = SparkSession.builder.appName(\"Spark MLlib\").getOrCreate()\n",
        "\n",
        "# Memuat dataset\n",
        "path_data = \"athlete_events.csv\"\n",
        "data = spark.read.csv(path_data, header=True, inferSchema=True)\n",
        "\n",
        "# Pra-pemrosesan data: Memilih kolom yang relevan\n",
        "kolom_dipilih = [\"Age\", \"Height\", \"Weight\", \"Sex\", \"Sport\", \"Medal\"]\n",
        "data = data.select(kolom_dipilih).dropna()\n",
        "\n",
        "# Mengkodekan variabel kategori\n",
        "indexers = [\n",
        "    StringIndexer(inputCol=\"Sex\", outputCol=\"SexIndex\"),\n",
        "    StringIndexer(inputCol=\"Sport\", outputCol=\"SportIndex\"),\n",
        "    StringIndexer(inputCol=\"Medal\", outputCol=\"label\")\n",
        "]\n",
        "for indexer in indexers:\n",
        "    data = indexer.fit(data).transform(data)\n",
        "\n",
        "data = data.withColumn(\"Age\", col(\"Age\").cast(\"double\")) \\\n",
        "           .withColumn(\"Height\", col(\"Height\").cast(\"double\")) \\\n",
        "           .withColumn(\"Weight\", col(\"Weight\").cast(\"double\")) \\\n",
        "           .withColumn(\"SexIndex\", col(\"SexIndex\").cast(\"double\")) \\\n",
        "           .withColumn(\"SportIndex\", col(\"SportIndex\").cast(\"double\"))\n",
        "\n",
        "# Menggabungkan fitur ke dalam kolom \"features\"\n",
        "kolom_fitur = [\"Age\", \"Height\", \"Weight\", \"SexIndex\", \"SportIndex\"]\n",
        "assembler = VectorAssembler(inputCols=kolom_fitur, outputCol=\"features\", handleInvalid=\"skip\")\n",
        "data = assembler.transform(data)\n",
        "\n",
        "data.show(5, truncate=False)\n",
        "data.select(\"Age\", \"Height\", \"Weight\").describe().show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feWetKflYzQ6",
        "outputId": "1b048930-0b2f-4c04-8836-9594cf067690"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+------+---+-------------+-----+--------+----------+-----+--------------------------+\n",
            "|Age |Height|Weight|Sex|Sport        |Medal|SexIndex|SportIndex|label|features                  |\n",
            "+----+------+------+---+-------------+-----+--------+----------+-----+--------------------------+\n",
            "|24.0|180.0 |80.0  |M  |Basketball   |NA   |0.0     |19.0      |0.0  |[24.0,180.0,80.0,0.0,19.0]|\n",
            "|23.0|170.0 |60.0  |M  |Judo         |NA   |0.0     |22.0      |0.0  |[23.0,170.0,60.0,0.0,22.0]|\n",
            "|21.0|185.0 |82.0  |F  |Speed Skating|NA   |1.0     |15.0      |0.0  |[21.0,185.0,82.0,1.0,15.0]|\n",
            "|21.0|185.0 |82.0  |F  |Speed Skating|NA   |1.0     |15.0      |0.0  |[21.0,185.0,82.0,1.0,15.0]|\n",
            "|25.0|185.0 |82.0  |F  |Speed Skating|NA   |1.0     |15.0      |0.0  |[25.0,185.0,82.0,1.0,15.0]|\n",
            "+----+------+------+---+-------------+-----+--------+----------+-----+--------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-------+------------------+------------------+------------------+\n",
            "|summary|               Age|            Height|            Weight|\n",
            "+-------+------------------+------------------+------------------+\n",
            "|  count|            205688|            205688|            205688|\n",
            "|   mean|25.056556532223563|175.36527167360273| 70.68080782544436|\n",
            "| stddev|5.4834217645253975|10.544748708871348|14.336306352706321|\n",
            "|    min|              11.0|             127.0|              25.0|\n",
            "|    max|              71.0|             226.0|             214.0|\n",
            "+-------+------------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Membagi data menjadi data pelatihan dan pengujian\n",
        "data_latih, data_uji = data.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Membuat model Logistic Regression\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "# Melatih model\n",
        "model = lr.fit(data_latih)\n",
        "\n",
        "# Mengevaluasi model\n",
        "prediksi = model.transform(data_uji)\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\")\n",
        "akurasi = evaluator.evaluate(prediksi)\n",
        "\n",
        "print(f\"Akurasi pada data uji: {akurasi:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwccGF0IZpQ-",
        "outputId": "2aacf565-76ef-4cde-8a07-4e70a49f5294"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akurasi pada data uji: 0.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Optimasi Hyperparameter Tuning\") \\\n",
        "    .config(\"spark.executor.memory\", \"2g\") \\\n",
        "    .config(\"spark.executor.cores\", \"2\") \\\n",
        "    .config(\"spark.driver.memory\", \"2g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "data_latih_sample = data_latih.sample(fraction=0.2, seed=42)\n",
        "\n",
        "# Model Logistic Regression\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
        "\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(lr.regParam, [0.1, 1.0]) \\\n",
        "    .addGrid(lr.elasticNetParam, [0.0, 0.5]) \\\n",
        "    .build()\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "\n",
        "crossval = CrossValidator(\n",
        "    estimator=lr,\n",
        "    estimatorParamMaps=paramGrid,\n",
        "    evaluator=evaluator,\n",
        "    numFolds=3\n",
        ")\n",
        "\n",
        "# Melatih model dengan dataset sampel\n",
        "model_cv = crossval.fit(data_latih_sample)\n",
        "\n",
        "# Mengevaluasi model pada data uji\n",
        "prediksi = model_cv.transform(data_uji)\n",
        "akurasi = evaluator.evaluate(prediksi)\n",
        "\n",
        "print(f\"Akurasi model terbaik setelah tuning: {akurasi:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CzMpys-k5Zf",
        "outputId": "fc80e044-23b3-473c-cd36-4fc486818f30"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akurasi model terbaik setelah tuning: 0.85\n"
          ]
        }
      ]
    }
  ]
}